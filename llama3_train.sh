python ./llama3_train.py --model_name unsloth/llama-3-8b-bnb-4bit --max_seq_length 2048 --r 16 --lora_alpha 16 --lora_dropout 0 --random_state 3407 --dataset_path /workspace/papering_qa/data/train.json --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --warmup_steps 5 --max_steps 19320 --learning_rate 0.0002 --logging_steps 100 --save_steps 500 --save_total_limit 3 --output_dir "/workspace/papering_qa/work_dir2"